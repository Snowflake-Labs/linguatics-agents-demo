{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Gateway Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent Gateway is a multi-agent framework that offers native support for Snowflake tools. \n",
    "\n",
    "The system can be configured to work with 3 types of tools:\n",
    "- Cortex Search Tool: For unstructured data analysis, which requires a standard RAG access pattern.\n",
    "- Cortex Analyst Tool: For supporting structured data analysis, which requires a Text2SQL access pattern.\n",
    "- Python Tool: For supporting custom user operations (using 3rd Party API's), which requires calling arbitrary python.\n",
    "\n",
    "This notebook walks through how to configure and run a system with all 3 types of tools. The demo is designed to illustrate how the agent can answer questions that require a divserse combination of tools (RAG,Text2SQL, Python, or a combination).\n",
    "\n",
    "Note that Agent Gateway does not configure the underlying Cortex Search or Cortex Analyst services for the user. Those services must be configured before initializing the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate with Snowpark + set token as environment variable for use by the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_gateway import Agent\n",
    "from agent_gateway.tools.logger import gateway_logger\n",
    "from agent_gateway.tools import CortexAnalystTool, PythonTool, CortexSearchTool\n",
    "from snowflake.snowpark import Session\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Loading environment variables...\")\n",
    "if not os.getenv(\"PRIVATE_KEY_PASSPHRASE\"):\n",
    "    raise ValueError(\"PRIVATE_KEY_PASSPHRASE environment variable is not set.\")\n",
    "\n",
    "\n",
    "if not os.getenv(\"SNOWFLAKE_DEFAULT_CONNECTION_NAME\"):\n",
    "    raise ValueError(\n",
    "        \"SNOWFLAKE_DEFAULT_CONNECTION_NAME environment variable is not set.\"\n",
    "    )\n",
    "\n",
    "connection_parameters = {\n",
    "    \"connection_name\": os.getenv(\"SNOWFLAKE_DEFAULT_CONNECTION_NAME\"),\n",
    "    \"private_key_file_pwd\": os.getenv(\"PRIVATE_KEY_PASSPHRASE\"),\n",
    "    \"database\": os.getenv(\"SNOWFLAKE_DATABASE\", \"kamesh_llm_demo\"),\n",
    "    \"user\": os.getenv(\"SNOWFLAKE_USER\", \"kameshs\"),\n",
    "    \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"schema\": \"DATA\",\n",
    "    \"warehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"COMPUTE_WH\"),\n",
    "}\n",
    "snowpark_session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_session.use_database(connection_parameters[\"database\"])\n",
    "snowpark_session.use_schema(connection_parameters[\"schema\"])\n",
    "snowpark_session.use_warehouse(connection_parameters[\"warehouse\"])\n",
    "snowpark_session.sql(\"ls @my_models\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sarvamai import SarvamAI\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "SARVAM_AI_TRANSLATE_MODEL = \"sarvam-translate:v1\"\n",
    "\n",
    "# Initialize SarvamAI with your API key\n",
    "if os.getenv(\"SARVAM_API_KEY\") is None:\n",
    "    sarvam_api_key = getpass.getpass(\"Enter your SarvamAI API key: \")\n",
    "else:\n",
    "    sarvam_api_key = os.getenv(\"SARVAM_API_KEY\")\n",
    "\n",
    "__client = SarvamAI(api_subscription_key=sarvam_api_key)\n",
    "\n",
    "\n",
    "def chunk_text(text, max_length=1000):\n",
    "    \"\"\"Splits text into chunks of at most max_length characters while preserving word boundaries.\"\"\"\n",
    "    chunks = []\n",
    "\n",
    "    while len(text) > max_length:\n",
    "        split_index = text.rfind(\" \", 0, max_length)  # Find the last space within limit\n",
    "        if split_index == -1:\n",
    "            split_index = max_length  # No space found, force split at max_length\n",
    "\n",
    "        chunks.append(text[:split_index].strip())  # Trim spaces before adding\n",
    "        text = text[split_index:].lstrip()  # Remove leading spaces for the next chunk\n",
    "\n",
    "    if text:\n",
    "        chunks.append(text.strip())  # Add the last chunk\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def lang_detect(question: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Detect the language of the given question text using SarvamAI's language identification service.\n",
    "\n",
    "    This function uses the SarvamAI client to identify the language of the input text and logs\n",
    "    the detected language code for debugging purposes.\n",
    "\n",
    "    Args:\n",
    "        question (str): The input text/question for which to detect the language.\n",
    "\n",
    "    Returns:\n",
    "        str | None: The detected language code (e.g., 'en', 'hi', 'ta', etc.) or None if detection fails.\n",
    "\n",
    "    Example:\n",
    "        >>> # Detect language of English text\n",
    "        >>> lang_code = lang_detect(\"What is the weather today?\")\n",
    "        >>> print(lang_code)\n",
    "        'en-IN'\n",
    "\n",
    "        >>> # Detect language of Tamil text\n",
    "        >>> lang_code = lang_detect(\"இன்று வானிலை எப்படி இருக்கிறது?\")\n",
    "        >>> print(lang_code)\n",
    "        'ta-IN'\n",
    "\n",
    "        >>> # Detect language of Hindi text\n",
    "        >>> lang_code = lang_detect(\"आज मौसम कैसा है?\")\n",
    "        >>> print(lang_code)\n",
    "        'hi-IN'\n",
    "    \"\"\"\n",
    "    response = __client.text.identify_language(input=question)\n",
    "    __lang_code = response.language_code\n",
    "    gateway_logger.log(\"DEBUG\", f\"Detected Language Code: {__lang_code}\\n\")\n",
    "\n",
    "    return __lang_code\n",
    "\n",
    "\n",
    "def translate(\n",
    "    lang_code: str,\n",
    "    question: str,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Translate text from a detected language to English using SarvamAI's translation service.\n",
    "\n",
    "    This function takes a language code and question text, logs the translation process,\n",
    "    and returns the English translation using the SarvamAI translation API.\n",
    "\n",
    "    Args:\n",
    "        lang_code (str): The source language code (e.g., 'hi-IN', 'ta-IN', 'te-IN').\n",
    "        question (str): The input text/question to be translated to English.\n",
    "\n",
    "    Returns:\n",
    "        str: The translated text in English.\n",
    "\n",
    "    Example:\n",
    "        >>> # Translate Hindi text to English\n",
    "        >>> hindi_text = \"आज मौसम कैसा है?\"\n",
    "        >>> english_translation = translate(\"hi-IN\", hindi_text)\n",
    "        >>> print(english_translation)\n",
    "        'How is the weather today?'\n",
    "\n",
    "        >>> # Translate Tamil text to English\n",
    "        >>> tamil_text = \"இன்று வானிலை எப்படி இருக்கிறது?\"\n",
    "        >>> english_translation = translate(\"ta-IN\", tamil_text)\n",
    "        >>> print(english_translation)\n",
    "        'How is the weather today?'\n",
    "\n",
    "        >>> # Translate Telugu text to English\n",
    "        >>> telugu_text = \"ఈరోజు వాతావరణం ఎలా ఉంది?\"\n",
    "        >>> english_translation = translate(\"te-IN\", telugu_text)\n",
    "        >>> print(english_translation)\n",
    "        'How is the weather today?'\n",
    "    \"\"\"\n",
    "    gateway_logger.log(\n",
    "        \"DEBUG\",\n",
    "        f\"\"\"\\n\n",
    "Identified Language Code: {lang_code} \\n \n",
    "Question: {question}\\n\"\"\",\n",
    "    )\n",
    "\n",
    "    response = __client.text.translate(\n",
    "        input=question,\n",
    "        source_language_code=lang_code,\n",
    "        target_language_code=\"en-IN\",\n",
    "        model=SARVAM_AI_TRANSLATE_MODEL,\n",
    "    )\n",
    "    translation = response.translated_text\n",
    "    gateway_logger.log(\"DEBUG\", f\"Translation:{translation}\\n\")\n",
    "    return translation\n",
    "\n",
    "\n",
    "def answer_translator(\n",
    "    answer: str,\n",
    "    lang_code: str,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Translate an English answer back to the original language using SarvamAI's translation service.\n",
    "\n",
    "    This function takes an English answer and translates it back to the specified target language,\n",
    "    logging the process for debugging purposes. It's typically used to translate responses back\n",
    "    to the user's original language after processing.\n",
    "\n",
    "    Args:\n",
    "        lang_code (str): The target language code to translate to (e.g., 'hi-IN', 'ta-IN', 'te-IN').\n",
    "        answer (str): The English text/answer to be translated to the target language.\n",
    "\n",
    "    Returns:\n",
    "        str: The translated text in the target language.\n",
    "\n",
    "    Example:\n",
    "        >>> # Translate English answer to Hindi\n",
    "        >>> english_answer = \"The weather today is sunny and warm.\"\n",
    "        >>> hindi_translation = answer_translator(\"hi-IN\", english_answer)\n",
    "        >>> print(hindi_translation)\n",
    "        'आज का मौसम धूप और गर्म है।'\n",
    "\n",
    "        >>> # Translate English answer to Tamil\n",
    "        >>> english_answer = \"Your support ticket has been resolved.\"\n",
    "        >>> tamil_translation = answer_translator(\"ta-IN\", english_answer)\n",
    "        >>> print(tamil_translation)\n",
    "        'உங்கள் ஆதரவு டிக்கெட் தீர்க்கப்பட்டது।'\n",
    "\n",
    "        >>> # Translate English answer to Telugu\n",
    "        >>> english_answer = \"Thank you for your inquiry.\"\n",
    "        >>> telugu_translation = answer_translator(\"te-IN\", english_answer)\n",
    "        >>> print(telugu_translation)\n",
    "        'మీ విచారణకు ధన్యవాదాలు।'\n",
    "    \"\"\"\n",
    "    gateway_logger.log(\"DEBUG\", f\"English answer: \\n{answer}\\n\")\n",
    "    response = __client.text.translate(\n",
    "        input=answer,\n",
    "        source_language_code=\"en-IN\",\n",
    "        target_language_code=lang_code,\n",
    "        mode=\"modern-colloquial\",\n",
    "        model=\"mayura:v1\",\n",
    "    )\n",
    "    translation = response.translated_text\n",
    "    gateway_logger.log(\"DEBUG\", f\"Translation:{translation}\\n\")\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snowflake Tool Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cortex Search Tool and the Cortex Analyst Tool need to be configured as follows. Note that a connection object is required for each config. In the case below we're using the same connection object for both because the services are both in the same account/database/schema. Users have the option to pass in different connection objects as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_config = {\n",
    "    \"semantic_model\": \"support_tickets.yaml\",\n",
    "    \"stage\": \"MY_MODELS\",\n",
    "    \"service_topic\": \"Customer support tickets model\",\n",
    "    \"data_description\": \"a table with customer support tickets\",\n",
    "    \"snowflake_connection\": snowpark_session,\n",
    "    \"max_results\": 5,\n",
    "}\n",
    "\n",
    "search_config = {\n",
    "    \"service_name\": \"INVOICE_SEARCH\",\n",
    "    \"service_topic\": \"Customer invoice related queries.\",\n",
    "    \"data_description\": \"Customer invoices and related documents\",\n",
    "    \"retrieval_columns\": [\"PARSED_TEXT\", \"URL\"],\n",
    "    \"snowflake_connection\": snowpark_session,\n",
    "    \"k\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Tool Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring a Python Tool for the Agent Gateway requires 1) Python Callable 2) Tool Description (what does the tool do) 3) Output Description (what does the tool output). \n",
    "\n",
    "In the example below we create a NewsTool object that submits a HTTP request to a 3rd Party News API. The python callable is passed into the Python Tool as `news_api_func`.To use the tool below get your free token by signing up for an account at thenewsapi.com or just create your own python function and pass it into the PythonTool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__language_identifier_config = {\n",
    "    \"tool_description\": \"Identify the language of the question\",\n",
    "    \"output_description\": \"It should identify the language code and return it for other tools to use.\",\n",
    "    \"python_func\": lang_detect,\n",
    "}\n",
    "\n",
    "__translator_config = {\n",
    "    \"tool_description\": \"Use the indentified language from the right tool (e.g., ta-IN for Tamil, hi-IN for Hindi, te-IN for Telugu) as the language code to translate the question to English and then pass the english question to Analyst tool.\",\n",
    "    \"output_description\": \"Returns English translation of the question\",\n",
    "    \"python_func\": translate,\n",
    "}\n",
    "\n",
    "__answer_translator_config = {\n",
    "    \"tool_description\": \"Translate the answer from English back to the original language of the question.\",\n",
    "    \"output_description\": \"Returns the answer translated from English to the original question's language.\",\n",
    "    \"python_func\": answer_translator,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_details = CortexSearchTool(**search_config)\n",
    "support_tickets = CortexAnalystTool(**analyst_config)\n",
    "lang_identifier = PythonTool(**__language_identifier_config)\n",
    "translator = PythonTool(**__translator_config)\n",
    "answer_translator = PythonTool(**__answer_translator_config)\n",
    "# margin_eval = SQLTool(**sql_tool_config)\n",
    "\n",
    "\n",
    "snowflake_tools = [lang_identifier, translator, support_tickets, answer_translator]\n",
    "# snowflake_tools = [translator,support_tickets,invoice_details,answer_translator]\n",
    "# agent = Agent(snowflake_connection=snowpark, tools=snowflake_tools, max_retries=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 types of questions below are designed to showcase the breadth of tool use patterns possible with the Agent Gateway. \n",
    "\n",
    "- The Structured Data Questions use the Cortex Analyst agent. \n",
    "- The Unstructured Data Questions use either the Cortex Search agent or the Python (News API) agent.\n",
    "- The last section includes a question that requires the use of both types of tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install framework with requisite dependencies with `pip install orchestration-framework[trulens]` and initialize as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from agent_gateway import TruAgent\n",
    "# from trulens.connectors.snowflake import SnowflakeConnector\n",
    "# from trulens.core.database.connector.default import DefaultDBConnector\n",
    "\n",
    "# connector = DefaultDBConnector()\n",
    "\n",
    "# connector = SnowflakeConnector(\n",
    "#     snowpark_session=snowpark_session,\n",
    "# )\n",
    "\n",
    "# agent = TruAgent(\n",
    "#     app_name=\"observable\",\n",
    "#     app_version=\"v0\",\n",
    "#     trulens_snowflake_connection=connector,\n",
    "#     snowflake_connection=snowpark_session,\n",
    "#     tools=snowflake_tools,\n",
    "#     max_retries=3,\n",
    "# )\n",
    "FUSION_FINISH = \"Finish\"\n",
    "FUSION_REPLAN = \"Replan\"\n",
    "# OUTPUT_PROMPT = \"\"\"\n",
    "#     \"You must solve the Question. You are given Observations and you can use them to solve the Question. \"\n",
    "#     \"Then you MUST provide a Thought, and then an Action. Do not use any parenthesis.\\n\"\n",
    "#     \"You will be given a question either some passages or numbers, which are observations.\\n\\n\"\n",
    "#     \"Thought step can reason about the observations in 1-2 sentences, and Action can be only one type:\\n\"\n",
    "#     f\" (1) {FUSION_FINISH}(answer): returns the answer and finishes the task using information you found from observations.\"\n",
    "#     f\" (2) {FUSION_REPLAN}: returns the original user's question, clarifying questions or comments about why it wasn't answered, and replans in order to get the information needed to answer the question.\"\n",
    "#     \"\\n\"\n",
    "#     \"Follow the guidelines that you will die if you don't follow:\\n\"\n",
    "#     \"  - Answer should be directly answer the question.\\n\"\n",
    "#     \"  - Thought should be 1-2 sentences.\\n\"\n",
    "#     \"  - Action can only be Finish or Replan\\n\"\n",
    "#     \"  - Action should be Finish if you have enough information to answer the question\\n\"\n",
    "#     \"  - Action Should be Replan if you don't have enough information to answer the question\\n\"\n",
    "#     \"  - You must say <END_OF_RESPONSE> at the end of your response.\\n\"\n",
    "#     \"  - If the user's question is too vague or unclear, say why and ask for clarification.\\n\"\n",
    "#     \"  - If the correct tool is used, but the information does not exist, then let the user know.\\n\"\n",
    "#     \"  - Ensure that langauge of the answer matches language of the user question.\\n\"\n",
    "#     \"\\n\"\n",
    "#     \"\\n\"\n",
    "#     \"Here are some examples:\\n\"\n",
    "#     \"\\n\"\n",
    "#     \"Question: What is the EBITDA of Berkshire Hathaway?\\n\"\n",
    "#     \"cortexanalyst('What is the EBITDA of Berkshire Hathaway?')\\n\"\n",
    "#     \"Observation:   SYMBOL                    SHORTNAME  START_DATE  END_DATE        EBITDA\\n0  BRK-B  Berkshire Hathaway Inc. New      413.72    413.72  107046002688\\n\"\n",
    "#     \"Thought: Berkshire Hathaway's latest EBITDA is $107,046,002,688, or $107 Billion.\\n\"\n",
    "#     f\"Action: {FUSION_FINISH}(Berkshire's latest EBITDA is $107 Billion.)\\n\"\n",
    "#     \"<END_OF_RESPONSE>\\n\"\n",
    "#     \"\\n\"\n",
    "#     \"\\n\"\n",
    "#     \"Question: What is the stock price of the neighborhood laundromat?\\n\"\n",
    "#     \"cortexanalyst('What is the EBITDA of the neighborhood laundromat?')\\n\"\n",
    "#     \"Observation: Apologies, but the question 'What is the market cap of the neighborhood laundromat?' is unclear because the company neighborhood laundromat is not specified in the provided semantic data model. The model does not include a company with the name neighborhood laundromate, and without additional information, it is not possible to determine which company the user is referring to.\\n\"\n",
    "#     \"Thought: The information requested does not exist in the available tools.\\n\"\n",
    "#     f\"Action: {FUSION_REPLAN}(No data is available for the neighborhood laundromat. Please consider rephrasing your request to be more specific, or contact your administrator to confirm that the system contains the relevant information.)\\n\"\n",
    "#     \"<END_OF_RESPONSE>\\n\"\n",
    "#     \"\\n\"\n",
    "#     \"\\n\"\n",
    "#     \"Question: What is the latest news about Berkshire Hathaway?\\n\"\n",
    "#     \"newstool(Berkshire Hathaway)\\n\"\n",
    "#     \"Observation: '[{'uuid': 'c177ede5-07a7-4f63-a3b7-52790c8fd08e', 'title': 'Berkshire Hathaway-berkshire Hathaway Inc -- Berkshire Says It H…', 'description': 'BERKSHIRE HATHAWAY-BERKSHIRE HATHAWAY INC -- BERKSHIRE SAYS IT HAD $147.4 BLN OF CASH AND EQUIVALENTS AS OF JUNE 30...', 'keywords': 'Markets', 'snippet': \\\"Berkshire Hathaway Inc. (Berkshire) is a holding company owning subsidiaries engaged in various business activities. Berkshire's various business activities inc...\",\n",
    "#     \"url': 'https://www.marketscreener.com/quote/stock/BERKSHIRE-HATHAWAY-INC-11915/news/BERKSHIRE-HATHAWAY-BERKSHIRE-HATHAWAY-INC-BERKSHIRE-SAYS-IT-H-8230-44531571/', 'image_url': 'https://www.marketscreener.com/images/twitter_MS_fdblanc.png', 'language': 'en', 'published_at': '2023-08-05T12:15:15.000000Z', 'source': 'marketscreener.com', 'categories': ['business'], 'relevance_score': 55.634586}, {'uuid': '56202cf0-38af-4a20-b411-994c92d8c7cd', 'title': 'Berkshire Hathaway Inc. (OTCMKTS:BRK-A) Major Shareholder Berkshire Hathaway Inc Acquires 716,355 Shares', 'description': 'Read Berkshire Hathaway Inc. (OTCMKTS:BRK-A) Major Shareholder Berkshire Hathaway Inc Acquires 716,355 Shares at ETF Daily News', 'keywords': 'Berkshire Hathaway, OTCMKTS:BRK-A, BRK-A, Financial Service, Insider Trading, Insider Trades, Stocks', 'snippet': 'Berkshire Hathaway Inc. (OTCMKTS:BRK-A – Get Rating) major shareholder Berkshire Hathaway Inc bought 716,355 shares of Berkshire Hathaway stock in a transacti...', 'url': 'https://www.etfdailynews.com/2022/05/13/berkshire-hathaway-inc-otcmktsbrk-a-major-shareholder-berkshire-hathaway-inc-acquires-716355-shares/', 'image_url': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/berkshire-hathaway-inc-logo.png?v=20211203153558&w=240&h=240&zc=2', 'language': 'en', 'published_at': '2022-05-13T11:18:50.000000Z', 'source': 'etfdailynews.com', 'categories': ['business'], 'relevance_score': 53.612434}]\"\n",
    "#     \"\\n\"\n",
    "#     \"Thought: The recent news about Berkshire Hathaway include information about its financials and recent activities.\\n\"\n",
    "#     f\"Action: {FUSION_FINISH}('Recent news about Berkshire Hathaways includes:\\n- Article: Berkshire Hathaway-Berkshire Hathaway Inc -- Berkshire Says It H…  Source: [Market Screener](https://www.marketscreener.com/quote/stock/BERKSHIRE-HATHAWAY-INC-11915/news/BERKSHIRE-HATHAWAY-BERKSHIRE-HATHAWAY-INC-BERKSHIRE-SAYS-IT-H-8230-44531571/) \\n - Article: Berkshire Hathaway Inc. (OTCMKTS:BRK-A) Major Shareholder Berkshire Hathaway Inc Acquires 716,355 Shares' Source: [ETF Daily News](https://www.etfdailynews.com/2022/05/13/berkshire-hathaway-inc-otcmktsbrk-a-major-shareholder-berkshire-hathaway-inc-acquires-716355-shares/)) '\\n\"\n",
    "#     \"<END_OF_RESPONSE>\\n\"\n",
    "#     \"\\n\"\n",
    "#     \"\\n\"\n",
    "#     \"Question: How many queries are processed on Snowflake's platform?\\n\"\n",
    "#     \"cortexsearch(How many queries are processed on Snowflake's platform?)\\n\"\n",
    "#     \"Observation: ['deliver the Data Cloud, enabling a consistent, global user experience.\\nOur platform supports a wide range of workloads that enable our customers’ most important business objectives, including data warehousing, data lakes, data engineering, data\\nscience, data application development, and data sharing. From January 1, 2022 to January 31, 2022, we processed an average of over 1,496 million daily queries across all of our\\ncustomer accounts, up from an average of over 777 million daily queries during the corresponding month of the prior fiscal year. We also recently launched our Powered by\\nSnowflake program to help companies build, operate, and grow applications in the Data Cloud by supporting developers across all stages of the application journey. Members of the\\nprogram have access to go-to-market, customer support, and engineering expertise.\\nWe have an industry-vertical focus, which allows us to go to market with tailored business solutions. For example, we have launched the Financial Services Data Cloud, the\\nMedia Data Cloud, the Healthcare and Life Sciences Data Cloud, and the Retail Data Cloud. Each of these Data Clouds brings together Snowflake’s platform capabilities with\\nindustry-specific partner solutions and datasets to drive business growth and deliver improved experiences and insights.\\nOur business benefits from powerful network effects. The Data Cloud will continue to grow as organizations move their siloed data from cloud-based repositories and on-\\npremises data centers to the Data Cloud. The more customers adopt our platform, the more data can be exchanged with other Snowflake customers, partners, data providers, and\\ndata consumers, enhancing the value of our platform for all users. We believe this network effect will help us drive our vision of the Data Cloud.\\n75/14/24, 8:55 AM snow-20220131\\nhttps://www.sec.gov/Archives/edgar/data/1640147/000164014722000023/snow-20220131.htm 8/183Table of Contents',\"\n",
    "#     \"'the Data Cloud, enabling a consistent, global user experience.\\nOur platform supports a wide range of workloads that enable our customers’ most important business objectives, including data warehouse, data lake, data engineering, AI/ML,\\napplications, collaboration, cybersecurity and Unistore. From January 1, 2024 to January 31, 2024, we processed an average of approximately 4.2 billion daily queries across all our\\ncustomer accounts, up from an average of approximately 2.6 billion daily queries during the corresponding month of the prior fiscal year. We are committed to expanding our\\nplatform’s use cases and supporting developers in building their applications and businesses. In 2021, we launched Snowpark for Java and Scala to allow developers to build in the\\nlanguage of their choice, and in 2022 we added support for Python. In 2023, we launched Snowpark Container Services, a fully managed container platform designed to facilitate\\nthe deployment, management, and scaling of containerized applications and AI/ML models within our ecosystem. We continue to invest in our Native Application program to help\\ncompanies build, operate, and market applications in the Data Cloud by supporting developers across all stages of the application journey.\\nWe have an industry-vertical focus, which allows us to go to market with tailored business solutions. For example, we have launched the Telecom Data Cloud, the Financial\\nServices Data Cloud, the Media Data Cloud, the Healthcare and Life Sciences Data Cloud, and the Retail Data Cloud. Each of these brings together Snowflake’s platform\\ncapabilities with industry-specific partner solutions and datasets to drive business growth and deliver improved experiences and insights.\\nOur business benefits from powerful network effects. The Data Cloud will continue to grow as organizations move their siloed data from cloud-based repositories and on-',\"\n",
    "#     \"'Our cloud-native architecture consists of three independently scalable but logically integrated layers across compute, storage, and cloud services. The compute layer provides\\ndedicated resources to enable users to simultaneously access common data sets for many use cases with minimal latency. The storage layer ingests massive amounts and varieties of\\nstructured, semi-structured, and unstructured data to create a unified data record. The cloud services layer intelligently optimizes each use case’s performance requirements with no\\nadministration. This architecture is built on three major public clouds across 38 regional deployments around the world. These deployments are generally interconnected to deliver\\nthe Data Cloud, enabling a consistent, global user experience.\\nOur platform supports a wide range of workloads that enable our customers’ most important business objectives, including data warehousing, data lakes, and Unistore, as well\\nas collaboration, data engineering, cybersecurity, data science and machine learning, and application development. From January 1, 2023 to January 31, 2023, we processed an\\naverage of approximately 2.6 billion daily queries across all our customer accounts, up from an average of approximately 1.5 billion daily queries during the corresponding month\\nof the prior fiscal year. We are committed to expanding our platform’s use cases and supporting developers in building their applications and businesses. In 2021, we launched\\nSnowpark for Java to allow developers to build in the language of their choice, and in 2022 we added support for Python. We continue to invest in our Powered by Snowflake\\nprogram to help companies build, operate, and market applications in the Data Cloud by supporting developers across all stages of the application journey. As of January 31, 2023,\\nwe had over 820 Powered by Snowflake registrants. Powered by Snowflake partners have access to go-to-market, customer support, and engineering expertise.',\"\n",
    "#     \"'performance comparable to a relational, structured representation.\\n•Query Capabilities. Our platform is engineered to query petabytes of data. It implements support for a large subset of the ANSI SQL standard for read operations and data\\nmodification operations. Our platform provides additional features, including:\\n◦Time travel. Our platform keeps track of all changes happening to a table, which enables customers to query previous versions based on their preferences. Customers\\ncan query as of a relative point in time or as of an absolute point in time. This has a broad array of use cases for customers, including error recovery, time-based\\nanalysis, and data quality checks.5/14/24, 8:59 AM snow-20210131\\nhttps://www.sec.gov/Archives/edgar/data/1640147/000164014721000073/snow-20210131.htm 17/193◦ Cloning. Our architecture enables us to offer zero-copy cloning, an operation by which entire tables, schemas, or databases can be duplicated—or cloned—without\\nhaving to copy or duplicate the underlying data. Our platform leverages the separation between cloud services and storage to be able to track independent clones of\\nobjects sharing the same physical copy of the underlying data. This enables a variety of customer use cases such as making copies of production data for data\\nscientists, creating custom snapshots in time, or testing data pipelines.\\n105/14/24, 8:59 AM snow-20210131\\nhttps://www.sec.gov/Archives/edgar/data/1640147/000164014721000073/snow-20210131.htm 18/193Table of Contents\\n•Compute Model. Our platform offers a variety of capabilities to operate on data, from ingestion to transformation, as well as rich query and analysis. Our compute services\\nare primarily presented to users in one of two models, either through explicit specification of compute clusters we call virtual warehouses or through a number of serverless\\nservices.',\"\n",
    "#     \"'performance.\\n◦ Metadata. When data is ingested, our platform automatically extracts and stores metadata to speed up query processing. It does so by collecting data distribution\\ninformation for all columns in every micro-partition.5/14/24, 8:55 AM snow-20240131\\nhttps://www.sec.gov/Archives/edgar/data/1640147/000164014724000101/snow-20240131.htm 20/217◦Semi-structured and unstructured data. In addition to structured, relational data, our platform supports semi-structured data, including JSON, Avro, and Parquet, and\\nunstructured data, including PDF documents, screenshots, recordings, and images. Data in these formats can be ingested and queried with performance comparable to a\\nrelational, structured representation.\\n135/14/24, 8:55 AM snow-20240131\\nhttps://www.sec.gov/Archives/edgar/data/1640147/000164014724000101/snow-20240131.htm 21/217Table of Contents\\n•Query Capabilities. Our platform is engineered to query petabytes of data. It implements support for a large subset of the ANSI SQL standard for read operations and data\\nmodification operations. Our platform provides additional features, including:\\n◦Time travel. Our platform keeps track of all changes happening to a table, which enables customers to query previous versions based on their preferences. Customers\\ncan query as of a relative point in time or as of an absolute point in time. This has a broad array of use cases for customers, including error recovery, time-based\\nanalysis, and data quality checks.\\n◦ Cloning. Our architecture enables us to offer zero-copy cloning, an operation by which entire tables, schemas, or databases can be duplicated—or cloned—without\\nhaving to copy or duplicate the underlying data. Our platform leverages the separation between cloud services and storage to be able to track independent clones of\\nobjects sharing the same physical copy of the underlying data. This enables a variety of customer use cases such as making copies of production data for data']\"\n",
    "#     \"Thought: Berkshire Hathaway's latest EBITDA is $107,046,002,688, or $107 Billion.\\n\"\n",
    "#     f\"Action: {FUSION_FINISH}(Based on January 2024 data, Snowflake processes an average of approximately 4.2 billion daily queries across all customer account. This is an increase up from an average of approximately 2.6 billion daily queries during the corresponding month of the prior year.)\\n\"\n",
    "#     \"<END_OF_RESPONSE>\\n\"\n",
    "#     \"\\n\"\n",
    "#     \"\\n\",\n",
    "# \"\"\"\n",
    "\n",
    "OUTPUT_PROMPT = \"\"\"\n",
    "You are a helpful multi lingual analyst who can answer about customer support tickets. Guidelines for your tasks will be:\n",
    "1. Translates the question from detected language to english\n",
    "2. Send the translated question to the CortexAnalystTool to get the answer.\n",
    "3. Translate the answer back to the original language of the question using the 'answer_translator' tool.\n",
    "4. The answer should start with a marker 'Action: Finish' and end with a marker '<END_OF_RESPONSE>' with actual answer between the start and end marker.\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "    snowflake_connection=snowpark_session,\n",
    "    agent_llm=\"claude-3-5-sonnet\",\n",
    "    tools=snowflake_tools,\n",
    "    fusion_prompt=OUTPUT_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Data Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = agent(\n",
    "    \"செல்லுலார் சேவை வகைக்கும் வணிக இணைய சேவை வகைக்கும் இடையிலான வாடிக்கையாளர் ஆதரவு டிக்கெட்டுகளின் விநியோகத்தை எனக்குக் காட்ட முடியுமா?\"\n",
    ")\n",
    "gateway_logger.log(\"DEBUG\", f\"Output: {out}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unstructured Data Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = agent(\"கிரிகோரி ரோமிங் கட்டணம் வசூலிக்கப்பட்டாரா?\")\n",
    "gateway_logger.log(\"INFO\", \"Agent Output:\")\n",
    "gateway_logger.log(\"INFO\", out[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = agent(\"గ్రెగొరీకి ఛార్జ్ చేసిన రోమింగ్ ఫీజు ఉందా, ఉంటే ఆ మొత్తం ఎంత మరియు ఏ ఇన్వాయిస్ అని చెప్పగలరా?\")\n",
    "gateway_logger.log(\"INFO\", \"Agent Output:\")\n",
    "gateway_logger.log(\"INFO\", out[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the dashboard to view traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trulens.core import TruSession\n",
    "# from trulens.dashboard import run_dashboard\n",
    "\n",
    "# session = TruSession(connector=connector)\n",
    "\n",
    "# run_dashboard(session, port=8084, force=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
